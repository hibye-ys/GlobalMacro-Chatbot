{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import os\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from tempfile import TemporaryDirectory\n",
    "from utils import load_yaml\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class STT:\n",
    "    def __init__(self, config, diarization_file=None, use_api=False):\n",
    "        self.diarization_file = diarization_file\n",
    "        self.use_api = use_api\n",
    "        self.device = config[\"settings\"][\"device\"]\n",
    "        self.category_id = config[\"settings\"][\"category_id\"]\n",
    "        self.base_path = config[\"settings\"][\"base_path\"]\n",
    "        self.folder_path = os.path.join(self.base_path, self.category_id)\n",
    "\n",
    "        self.torch_dtype = (\n",
    "            torch.float16\n",
    "            if self.device == \"mps\" or self.device == \"cuda\"\n",
    "            else torch.float32\n",
    "        )\n",
    "        self.model_id = \"openai/whisper-large-v3\"\n",
    "        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            self.model_id,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "\n",
    "    def transcribe_with_speaker_diarization(self):\n",
    "        diarization_pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization\", use_auth_token=\"YOUR_HF_TOKEN\"\n",
    "        )\n",
    "        diarization = diarization_pipeline(self.diarization_file)\n",
    "\n",
    "        transcription_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\", model=\"openai/whisper-large-v3\"\n",
    "        )\n",
    "        audio = AudioSegment.from_file(self.diarization_file)\n",
    "\n",
    "        results = []\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            # 해당 부분의 오디오 추출\n",
    "            start_time = int(turn.start * 1000)  # ms로 변환\n",
    "            end_time = int(turn.end * 1000)  # ms로 변환\n",
    "            segment = audio[start_time:end_time]\n",
    "\n",
    "            # 임시 파일로 저장 (Whisper가 파일 경로를 요구하므로)\n",
    "            segment.export(\"temp_segment.wav\", format=\"wav\")\n",
    "\n",
    "            # Whisper로 전사\n",
    "            transcript = transcription_pipeline(\"temp_segment.wav\")[\"text\"]\n",
    "\n",
    "            # 결과 저장\n",
    "            results.append(\n",
    "                {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": turn.start,\n",
    "                    \"end\": turn.end,\n",
    "                    \"text\": transcript,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def whisper_api(self, file_path, segment_length_ms):\n",
    "        client = OpenAI()\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "        audio_length_ms = len(audio)\n",
    "\n",
    "        if len(audio) < segment_length_ms:\n",
    "            return audio\n",
    "\n",
    "        txt_list = []\n",
    "        with TemporaryDirectory() as tempfile:\n",
    "            for i, start in enumerate(range(0, audio_length_ms, segment_length_ms)):\n",
    "                end = min(start + segment_length_ms, audio_length_ms)\n",
    "                segment = audio[start:end]\n",
    "                segment.export(f\"{tempfile}/audio_{i}.mp3\", format=\"mp3\")\n",
    "                audio_file = open(f\"{tempfile}/audio_{i}.mp3\", \"rb\")\n",
    "                transcription = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\", file=audio_file, response_format=\"text\"\n",
    "                )\n",
    "\n",
    "                txt_list.append(transcription)\n",
    "\n",
    "        return \" \".join(txt_list)\n",
    "\n",
    "    def run_stt(self):\n",
    "        self.model.to(self.device)\n",
    "        processor = AutoProcessor.from_pretrained(self.model_id)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=128,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=16,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        audio_list = [\n",
    "            x for x in os.listdir(f\"{self.folder_path}/audio\") if not x.startswith(\".\")\n",
    "        ]\n",
    "\n",
    "        print(f\"총 오디오파일의 개수는 {len(audio_list)}개 입니다.\")\n",
    "\n",
    "        for audio in audio_list:\n",
    "            if f\"{audio.split('.')[0]}.txt\" in os.listdir(\n",
    "                f\"{self.folder_path}/origin_txt\"\n",
    "            ):\n",
    "                print(f\"{audio}는 이미 처리된 파일입니다\")\n",
    "                continue\n",
    "\n",
    "            audio_path = os.path.join(self.folder_path, \"audio\", audio)\n",
    "\n",
    "            if self.use_api:\n",
    "                segment_length_ms = 5 * 60 * 1000\n",
    "                result = self.whisper_api(audio_path, segment_length_ms)\n",
    "\n",
    "            else:\n",
    "                result = pipe(audio_path)[\"text\"]\n",
    "\n",
    "            with open(\n",
    "                f\"{self.folder_path}/origin_txt/{audio.split('.')[0]}.txt\",\n",
    "                \"w\",\n",
    "                encoding=\"utf-8\",\n",
    "            ) as file:\n",
    "                file.write(result)\n",
    "            print(f\"{audio}가 txt로 변환되었습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml(\"../config/stt.yaml\")\n",
    "stt = STT(config, use_api=True)\n",
    "\n",
    "# stt 수행\n",
    "stt.run_stt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-CK8n2Lx5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
