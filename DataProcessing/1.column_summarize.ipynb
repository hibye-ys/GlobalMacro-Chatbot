{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import SimpleJsonOutputParser\n",
    "import yaml\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from utils import load_yaml\n",
    "\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        summary_type=\"reduce\",\n",
    "        use_cod=True,\n",
    "        sm_chunker=False,\n",
    "        test_count=0,\n",
    "    ):\n",
    "        self.sm_chunker = sm_chunker\n",
    "        self.summary_type = summary_type\n",
    "        self.filetype = config[\"settings\"][\"filetype\"]\n",
    "        self.test_count = test_count\n",
    "        self.use_cod = use_cod\n",
    "        self.base_path = config[\"settings\"][\"base_path\"]\n",
    "        self.edit_path = config[\"settings\"][\"edit_path\"]\n",
    "        self.category_id = config[\"settings\"][\"category_id\"]\n",
    "        self.raw_path = f\"{self.base_path}/{self.category_id}/{self.filetype}\"\n",
    "        self.output_path = f\"{self.edit_path}/{self.category_id}/{self.filetype}\"\n",
    "\n",
    "        self.llm = ChatOpenAI(temperature=0.1, model=\"gpt-4o-mini-2024-07-18\")\n",
    "        self.langsmith = f\"{self.category_id}Summary\"\n",
    "\n",
    "        self.cod_prompt_template = self.load_yaml(\n",
    "            \"../prompts/summarization/chain_of_density.yaml\"\n",
    "        )[\"knowledge_graph_prompt2\"]\n",
    "        self.map_prompt_template = self.load_yaml(\n",
    "            \"../prompts/summarization/map_prompt.yaml\"\n",
    "        )[\"prompt\"]\n",
    "        self.reduce_prompt_template = self.load_yaml(\n",
    "            \"../prompts/summarization/reduce_prompt.yaml\"\n",
    "        )[\"prompt\"]\n",
    "        self.refine_prompt_template = self.load_yaml(\n",
    "            \"../prompts/summarization/refine_prompt.yaml\"\n",
    "        )[\"prompt\"]\n",
    "\n",
    "        self.cod_prompt = PromptTemplate.from_template(self.cod_prompt_template)\n",
    "        self.map_prompt = PromptTemplate.from_template(self.map_prompt_template)\n",
    "        self.reduce_prompt = PromptTemplate.from_template(self.reduce_prompt_template)\n",
    "        self.refine_prompt = PromptTemplate.from_template(self.refine_prompt_template)\n",
    "\n",
    "        self.map_chain = self.map_prompt | self.llm | StrOutputParser()\n",
    "        self.reduce_chain = self.reduce_prompt | self.llm | StrOutputParser()\n",
    "        self.refine_chain = self.refine_prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def load_yaml(self, file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "    def split_MDdocs(self, filename):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "            (\"####\", \"Header 4\"),\n",
    "        ]\n",
    "\n",
    "        splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "        )\n",
    "        split_MDdocs = splitter.split_text(md_content)\n",
    "\n",
    "        num_tokens = self.llm.get_num_tokens(md_content)\n",
    "\n",
    "        print(f\"document의 token 수는 {num_tokens}개 입니다\")\n",
    "        print(f\"document의 split 개수는 {len(split_MDdocs)}개 입니다\")\n",
    "\n",
    "        return split_MDdocs\n",
    "\n",
    "    def split_TXTdocs(self, filename):\n",
    "        loader = TextLoader(filename)\n",
    "        doc = loader.load()\n",
    "        full_text = doc[0].page_content\n",
    "\n",
    "        if self.sm_chunker:\n",
    "            splitter = SemanticChunker(\n",
    "                OpenAIEmbeddings(),\n",
    "                breakpoint_threshold_type=\"percentile\",\n",
    "                breakpoint_threshold_amount=70,\n",
    "            )\n",
    "            split_TXTdocs = splitter.create_documents([full_text])\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "            split_TXTdocs = splitter.split_documents(doc)\n",
    "        num_tokens = self.llm.get_num_tokens(full_text)\n",
    "\n",
    "        print(f\"document의 token 수는 {num_tokens}개 입니다\")\n",
    "        print(f\"document의 split 개수는 {len(split_TXTdocs)}개 입니다\")\n",
    "\n",
    "        return split_TXTdocs\n",
    "\n",
    "    def directory_exists(self, output_path):\n",
    "        directory = os.path.dirname(output_path)\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    def map_summaries_to_file(self, summary, title):\n",
    "        path = f\"{self.output_path}/{title}_MapSummary{self.test_count}.{self.filetype}\"\n",
    "        with open(path, \"a\", encoding=\"utf-8\") as file:\n",
    "            file.write(summary + \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\")\n",
    "\n",
    "    def chain_of_density(self, docs, title):\n",
    "        cod_chain_inputs = {\n",
    "            \"content\": lambda d: d.get(\"content\"),\n",
    "            \"content_category\": lambda d: d.get(\n",
    "                \"content_category\", \"Financial Article\"\n",
    "            ),\n",
    "            \"entity_range\": lambda d: d.get(\"entity_range\", \"1-3\"),\n",
    "            \"max_words\": lambda d: int(d.get(\"max_words\", 200)),\n",
    "            \"iterations\": lambda d: int(d.get(\"iterations\", 5)),\n",
    "        }\n",
    "\n",
    "        cod_chain = (\n",
    "            cod_chain_inputs | self.cod_prompt | self.llm | SimpleJsonOutputParser()\n",
    "        )\n",
    "\n",
    "        cod_docs = []\n",
    "        for content in docs:\n",
    "            # 결과를 저장할 빈 리스트 초기화\n",
    "            results: list[dict[str, str]] = []\n",
    "\n",
    "            # cod_chain을 스트리밍 모드로 실행하고 부분적인 JSON 결과를 처리\n",
    "            for partial_json in cod_chain.stream(\n",
    "                {\"content\": content, \"content_category\": \"Financial Article\"}\n",
    "            ):\n",
    "                # 각 반복마다 results를 업데이트\n",
    "                results = partial_json\n",
    "\n",
    "                # 현재 결과를 같은 줄에 출력 (캐리지 리턴을 사용하여 이전 출력을 덮어씀)\n",
    "                print(results, end=\"\\r\", flush=True)\n",
    "\n",
    "            # 총 요약 수 계산\n",
    "            total_summaries = len(results)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # 각 요약을 순회하며 처리\n",
    "            i = 1\n",
    "            for cod in results:\n",
    "                # 누락된 엔티티들을 추출하고 포맷팅\n",
    "                added_entities = \", \".join(\n",
    "                    [\n",
    "                        ent.strip()\n",
    "                        for ent in cod.get(\n",
    "                            \"missing_entities\", 'ERR: \"missing_entiies\" key not found'\n",
    "                        ).split(\";\")\n",
    "                    ]\n",
    "                )\n",
    "                # 더 밀도 있는 요약 추출\n",
    "                summary = cod.get(\"denser_summary\", 'ERR: missing key \"denser_summary\"')\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            self.map_summaries_to_file(summary, title)\n",
    "            cod_docs.append(summary)\n",
    "\n",
    "        if self.summary_type == \"refine\":\n",
    "            previous_summary = cod_docs[0]\n",
    "            for current_summary in cod_docs[1:]:\n",
    "\n",
    "                previous_summary = self.refine_chain.invoke(\n",
    "                    {\n",
    "                        \"previous_summary\": previous_summary,\n",
    "                        \"current_summary\": current_summary,\n",
    "                        \"language\": \"Korean\",\n",
    "                    }\n",
    "                )\n",
    "            return previous_summary\n",
    "\n",
    "        elif self.summary_type == \"reduce\":\n",
    "            return self.reduce_chain.invoke(\n",
    "                {\"doc_summaries\": cod_docs, \"language\": \"Korean\"}\n",
    "            )\n",
    "\n",
    "    def map_reduce_chain(self, docs):\n",
    "        doc_summaries = [\n",
    "            self.map_chain.invoke({\"documents\": doc, \"language\": \"Korean\"})\n",
    "            for doc in docs\n",
    "        ]\n",
    "        return self.reduce_chain.invoke(\n",
    "            {\"doc_summaries\": doc_summaries, \"language\": \"Korean\"}\n",
    "        )\n",
    "\n",
    "    def map_refine_chain(self, docs):\n",
    "        input_doc = [\n",
    "            {\"documents\": doc.page_content, \"language\": \"Korean\"} for doc in docs\n",
    "        ]\n",
    "\n",
    "        doc_summaries = self.map_chain.batch(input_doc)\n",
    "\n",
    "        previous_summary = doc_summaries[0]\n",
    "\n",
    "        for current_summary in doc_summaries[1:]:\n",
    "\n",
    "            previous_summary = self.refine_chain.invoke(\n",
    "                {\n",
    "                    \"previous_summary\": previous_summary,\n",
    "                    \"current_summary\": current_summary,\n",
    "                    \"language\": \"Korean\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return previous_summary\n",
    "\n",
    "    def run_chain(self):\n",
    "        self.directory_exists(f\"{self.output_path}/\")\n",
    "\n",
    "        raw_files = set(\n",
    "            x.split(f\".{self.filetype}\")[0] for x in os.listdir(self.raw_path)\n",
    "        )\n",
    "        edit_files = set(\n",
    "            x.split(f\"_{self.summary_type}\")[0] for x in os.listdir(self.output_path)\n",
    "        )\n",
    "\n",
    "        new_files = raw_files - edit_files\n",
    "        print(new_files)\n",
    "        if not new_files:\n",
    "            print(\"업데이트할 파일이 없습니다\")\n",
    "\n",
    "        # logging.langsmith(self.langsmith)\n",
    "\n",
    "        for file_name in list(new_files):\n",
    "            raw_file_path = os.path.join(self.raw_path, file_name + f\".{self.filetype}\")\n",
    "\n",
    "            if self.filetype == \"md\":\n",
    "                splitted_docs = self.split_MDdocs(raw_file_path)\n",
    "            elif self.filetype == \"txt\":\n",
    "                splitted_docs = self.split_TXTdocs(raw_file_path)\n",
    "\n",
    "            if self.use_cod:\n",
    "                summary_result = self.chain_of_density(splitted_docs, file_name)\n",
    "            elif self.summary_type == \"refine\" and not self.use_cod:\n",
    "                summary_result = self.map_refine_chain(splitted_docs)\n",
    "            elif self.summary_type == \"reduce\" and not self.use_cod:\n",
    "                summary_result = self.map_reduce_chain(splitted_docs)\n",
    "\n",
    "            output_file_path = f\"{self.edit_path}/{self.category_id}/{self.filetype}/{file_name}_{self.summary_type}Summary{self.test_count}.{self.filetype}\"\n",
    "\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(summary_result)\n",
    "\n",
    "            print(f\"{raw_file_path} 요약이 완료되었습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml(\"../config/summarizer.yaml\")\n",
    "summarizer = Summarizer(\n",
    "    config, summary_type=\"refine\", use_cod=True, sm_chunker=False, test_count=0\n",
    ")\n",
    "summarizer.run_chain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
