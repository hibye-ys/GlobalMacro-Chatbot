{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_teddynote import logging\n",
    "import yaml\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class SentenceCorrection:\n",
    "    def __init__(self, config):\n",
    "        self.category_id = config[\"settings\"][\"category_id\"]\n",
    "        self.base_path = config[\"settings\"][\"base_path\"]\n",
    "        self.folder_path = os.path.join(self.base_path, self.category_id)\n",
    "        self.llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4o-mini-2024-07-18\")\n",
    "        self.prompt_template = self.load_yaml(\n",
    "            \"../prompts/summarization/sentence_correction.yaml\"\n",
    "        )[\"prompt\"]\n",
    "        self.prompt = PromptTemplate.from_template(self.prompt_template)\n",
    "\n",
    "    def load_yaml(self, file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "    def save_correction_file(self, txt, answer, file_path):\n",
    "        txt_path = os.path.join(file_path, txt)\n",
    "        with open(txt_path, \"a\", encoding=\"utf-8\") as file:\n",
    "            file.write(answer)\n",
    "\n",
    "    def run(self):\n",
    "        # logging.langsmith(f\"TextCorrection{self.category_id.upper()}\")\n",
    "\n",
    "        chain = self.prompt | self.llm | StrOutputParser()\n",
    "        origin_txt_path = os.path.join(self.folder_path, \"origin_txt\")\n",
    "        eidted_txt_path = os.path.join(self.folder_path, \"txt\")\n",
    "        txts = [x for x in os.listdir(origin_txt_path) if x.endswith(\"txt\")]\n",
    "        edited_txt_list = [x for x in os.listdir(eidted_txt_path) if x.endswith(\"txt\")]\n",
    "\n",
    "        for txt in txts:\n",
    "            if txt in edited_txt_list:\n",
    "                print(f\"이미 {txt} 가 존재 합니다\")\n",
    "                continue\n",
    "\n",
    "            loader = TextLoader(f\"{origin_txt_path}/{txt}\")\n",
    "            docs = loader.load()\n",
    "\n",
    "            splitter = CharacterTextSplitter(\n",
    "                separator=\". \", chunk_size=5000, chunk_overlap=0, length_function=len\n",
    "            )\n",
    "            split_docs = splitter.split_documents(docs)\n",
    "\n",
    "            inputs = [{\"docs\": chunk} for chunk in split_docs]\n",
    "            corrected_texts = chain.batch(inputs)\n",
    "\n",
    "            final_corrected_text = \"\\n\".join(corrected_texts)\n",
    "\n",
    "            self.save_correction_file(txt, final_corrected_text, eidted_txt_path)\n",
    "            print(f\"제목 '{txt}'에 대한 교정된 파일이 추가되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_yaml\n",
    "\n",
    "\n",
    "config = load_yaml(\"../config/sentence_correction.yaml\")\n",
    "sc = SentenceCorrection(config)\n",
    "sc.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
