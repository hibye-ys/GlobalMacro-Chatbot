{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime\n",
    "from utils import load_yaml\n",
    "from extract_graphstate import (\n",
    "    extract_documents_for_multivectorstore,\n",
    "    extract_documents_for_singlestore,\n",
    "    extract_documents_for_multidocstore,\n",
    ")\n",
    "\n",
    "config = load_yaml(\"../config/embedding.yaml\")\n",
    "category_id = config[\"settings\"][\"category_id\"]\n",
    "filetype = config[\"settings\"][\"filetype\"]\n",
    "edit_path = config[\"settings\"][\"edit_path\"]\n",
    "# path = os.path.join(edit_path, category_id, \"json\")\n",
    "output_path = config[\"settings\"][\"output_path\"]\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_json_files(path):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for category in config[\"settings\"][\"category_id\"]:\n",
    "    category_path = os.path.join(edit_path, category, \"json\")\n",
    "    data_list.extend(load_json_files(category_path))\n",
    "\n",
    "\n",
    "all_documents = []\n",
    "for data in data_list:\n",
    "    documents = extract_documents_for_singlestore(data)\n",
    "    all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_documents), all_documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metakeys = []\n",
    "for i in range(len(all_documents)):\n",
    "    metakeys.extend(list(all_documents[i].metadata.keys()))\n",
    "\n",
    "metadata_keys = set(metakeys)\n",
    "metadata_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import preprocess_documents\n",
    "\n",
    "contents, metadatas = preprocess_documents(\n",
    "    split_docs=all_documents,\n",
    "    metadata_keys=metadata_keys,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(metakeys):\n",
    "    metadatas[key] = [header if header is not None else \"\" for header in metadatas[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2012, 2012)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents), len(metadatas[\"Header 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token 수 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 수가 4000이 넘는 문서수는 0개 입니다\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "i = 0\n",
    "for docs in all_documents:\n",
    "    token = llm.get_num_tokens(docs.page_content)\n",
    "\n",
    "    if token >= 4000:\n",
    "        print(f\"{docs.metadata['doc_id']}의 토큰수는 {token}개 이다\")\n",
    "        i += 1\n",
    "\n",
    "print(f\"토큰 수가 4000이 넘는 문서수는 {i}개 입니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from datetime import datetime\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "time = datetime.now().strftime(\"%Y.%m.%d\")\n",
    "passage_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "#passage_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "proj_name = f\"{time}_single_store\"\n",
    "store = LocalFileStore(f\"../cache/{proj_name}/data\")\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=passage_embeddings,\n",
    "    document_embedding_cache=store,\n",
    "    namespace=passage_embeddings.model,\n",
    ")\n",
    "\n",
    "DB_PATH = f\"../db/{time}_singlestore_chroma_db\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "db = Chroma(persist_directory=DB_PATH, embedding_function=cached_embedder)\n",
    "\n",
    "for i in tqdm(range(0, len(all_documents), BATCH_SIZE)):\n",
    "    batch = all_documents[i : i + BATCH_SIZE]\n",
    "    filtered = filter_complex_metadata(documents=batch)\n",
    "    db.add_documents(filtered)\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_teddynote.community.pinecone import create_index\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pc_index = create_index(\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    index_name=\"globalmacro-chatbot\",\n",
    "    dimension=4096,  #  (OpenAIEmbeddings: 1536, UpstageEmbeddings: 4096)\n",
    "    metric=\"dotproduct\",  # (dotproduct, euclidean, cosine)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import (\n",
    "    create_sparse_encoder,\n",
    "    fit_sparse_encoder,\n",
    ")\n",
    "from langchain_teddynote.korean import stopwords\n",
    "\n",
    "sparse_encoder = create_sparse_encoder(stopwords(), mode=\"kiwi\")\n",
    "\n",
    "saved_path = fit_sparse_encoder(\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    contents=contents,\n",
    "    save_path=\"../data/sparse_encoder_01.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from langchain_core.embeddings import Embeddings\n",
    "import secrets\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_hash() -> str:\n",
    "    \"\"\"24자리 무작위 hex 값을 생성하고 6자리씩 나누어 '-'로 연결합니다.\"\"\"\n",
    "    random_hex = secrets.token_hex(12)\n",
    "    return \"-\".join(random_hex[i : i + 6] for i in range(0, 24, 6))\n",
    "\n",
    "\n",
    "def upsert_documents(\n",
    "    index: Any,\n",
    "    namespace: str,\n",
    "    contents: List[str],\n",
    "    metadatas: List[Dict],\n",
    "    sparse_encoder: BM25Encoder,\n",
    "    embedder: Embeddings,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    load_dotenv()\n",
    "    ids = [generate_hash() for _ in range(len(contents))]\n",
    "\n",
    "    for i in range(0, len(contents), batch_size):\n",
    "        batch_contents = contents[i : i + batch_size]\n",
    "        batch_metadatas = metadatas[i : i + batch_size]\n",
    "        batch_ids = ids[i : i + batch_size]\n",
    "\n",
    "        dense_embeds = embedder.embed_documents(batch_contents)\n",
    "        sparse_embeds = sparse_encoder.encode_documents(batch_contents)\n",
    "\n",
    "        vectors = [\n",
    "            {\n",
    "                \"id\": _id,\n",
    "                \"sparse_values\": sparse,\n",
    "                \"values\": dense,\n",
    "                \"metadata\": metadata,\n",
    "            }\n",
    "            for _id, sparse, dense, metadata in zip(\n",
    "                batch_ids, sparse_embeds, dense_embeds, batch_metadatas\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        index.upsert(vectors=vectors, namespace=namespace)\n",
    "\n",
    "        print(f\"[upsert_documents] 배치 {i//batch_size + 1} 완료\")\n",
    "\n",
    "    print(f\"[upsert_documents]\\n{index.describe_index_stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sparse_encoder = load_sparse_encoder(\"../data/sparse_encoder_01.pkl\")\n",
    "upstage_embeddings = UpstageEmbeddings(\n",
    "    model=\"solar-embedding-1-large-passage\", api_key=os.environ[\"UPSTAGE_API_KEY\"]\n",
    ")\n",
    "\n",
    "\n",
    "upsert_documents(\n",
    "    index=pc_index,\n",
    "    namespace=\"financical-data-01\",\n",
    "    contents=contents,\n",
    "    metadatas=metadatas,\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    embedder=upstage_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.community.pinecone import upsert_documents_parallel\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote.community.pinecone import load_sparse_encoder\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sparse_encoder = load_sparse_encoder(\"../data/sparse_encoder_01.pkl\")\n",
    "upstage_embeddings = UpstageEmbeddings(\n",
    "    model=\"solar-embedding-1-large-passage\", api_key=os.environ[\"UPSTAGE_API_KEY\"]\n",
    ")\n",
    "upsert_documents_parallel(\n",
    "    index=pc_index,\n",
    "    namespace=\"financical-data-00\",\n",
    "    contents=contents,\n",
    "    metadatas=metadatas,\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    embedder=upstage_embeddings,\n",
    "    batch_size=32,\n",
    "    max_workers=30,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
