{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.make corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use splited data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from DataProcessing.utils import load_yaml\n",
    "from DataProcessing.extract_graphstate import (\n",
    "    extract_documents_for_docstore,\n",
    "    extract_documents_for_single_store,\n",
    "    extract_documents_for_vectorstore,\n",
    ")\n",
    "from autorag.data.corpus import langchain_documents_to_parquet\n",
    "\n",
    "config = load_yaml(\"../config/embedding.yaml\")\n",
    "category_id = config[\"settings\"][\"category_id\"]\n",
    "filetype = config[\"settings\"][\"filetype\"]\n",
    "edit_path = config[\"settings\"][\"edit_path\"]\n",
    "# path = os.path.join(edit_path, category_id, \"json\")\n",
    "output_path = config[\"settings\"][\"output_path\"]\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_json_files(path):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for category in config[\"settings\"][\"category_id\"]:\n",
    "    category_path = os.path.join(edit_path, category, \"json\")\n",
    "    data_list.extend(load_json_files(category_path))\n",
    "\n",
    "\n",
    "all_documents = []\n",
    "for data in data_list:\n",
    "    documents = extract_documents_for_single_store(data)\n",
    "    all_documents.extend(documents)\n",
    "\n",
    "corpus_df = langchain_documents_to_parquet(\n",
    "    all_documents, \"./data/use_splited_corpus.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "corpus_df = pd.read_parquet(\"./data/use_splited_corpus.parquet\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 행 수: 769\n"
     ]
    }
   ],
   "source": [
    "filtered_df = corpus_df[\n",
    "    corpus_df[\"contents\"].apply(lambda x: 30 < llm.get_num_tokens(x) < 500)\n",
    "]\n",
    "filtered_df.to_parquet(\"./data/filtered_corpus.parquet\")\n",
    "print(f\"저장된 행 수: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Generation QA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use AutoRAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from autorag.data.qacreation import generate_qa_llama_index, make_single_content_qa\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "prompt = \"\"\"The given text is a financial columns written in Korean.\n",
    "Generate question and answer pairs related to financial events, considering the importance of dates, timelines, and the sequence of events. Focus on financial concepts, the timing of the events, and how specific dates or time periods impact the content.\n",
    "\n",
    "Passage:\n",
    "{{text}}\n",
    "\n",
    "Number of questions to generate: {{num_questions}}\n",
    "\n",
    "Guidelines:\n",
    "1. Ensure that the questions are relevant to specific dates or timeframes mentioned in the passage.\n",
    "2. Include questions that ask about the significance of key financial events, focusing on when they occurred and their subsequent impact.\n",
    "3. The answers should provide precise information related to both the event and the date/time context.\n",
    "4. Be clear about the source (whose claim it is)\n",
    "5. Please write the result in Korean\n",
    "\n",
    "Example:\n",
    "[Q]: On what date did the stock market experience a significant drop?\n",
    "[A]: The stock market experienced a significant drop on March 9, 2020.\n",
    "\n",
    "[Q]: How did the interest rate change on July 15, 2021, affect the housing market?\n",
    "[A]: The interest rate increase on July 15, 2021, caused housing prices to stabilize as borrowing costs rose.\n",
    "\n",
    "Result:\n",
    "\"\"\"\n",
    "\n",
    "corpus_df = pd.read_parquet(\"./data/filtered_corpus.parquet\")\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.56)\n",
    "\n",
    "\n",
    "qa_df = make_single_content_qa(\n",
    "    corpus_df.sample(n=250),\n",
    "    content_size=100,\n",
    "    qa_creation_func=generate_qa_llama_index,\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    question_num_per_content=2,\n",
    "    output_filepath=\"./data/use_splited_qa_autorag.parquet\",\n",
    "    upsert=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use RAGAS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorag.data.qacreation.ragas import generate_qa_ragas\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.25,\n",
    "    reasoning: 0.25,\n",
    "    multi_context: 0.25,\n",
    "    conditional: 0.25,\n",
    "}\n",
    "\n",
    "corpus_df = pd.read_parquet(\"./data/filtered_corpus.parquet\")\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.56)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "qa_df = generate_qa_ragas(\n",
    "    corpus_df.sample(n=200),\n",
    "    test_size=10,\n",
    "    distributions=distributions,\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embedding_model=embedding_model,\n",
    ")\n",
    "qa_df.to_parquet(\"./data/use_splited_qa_ragas2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorag.evaluator import Evaluator\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "import autorag\n",
    "from langchain_upstage.embeddings import UpstageEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "# autorag.embedding_models[\"upstage_embed\"] = autorag.LazyInit(UpstageEmbeddings)\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    qa_data_path=\"./data/autorag_testset_100_0.parquet\",\n",
    "    corpus_data_path=\"./data/use_splited_corpus.parquet\",\n",
    "    project_dir=\"./benchmark/test_2\",\n",
    ")\n",
    "evaluator.start_trial(\"./config/retriever_test.yaml\")\n",
    "# evaluator.restart_trial(trial_path='your/path/to/trial_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorag.deploy import extract_best_config\n",
    "\n",
    "trial_path = \"./benchmark/test_1/4\"\n",
    "pipeline_dict = extract_best_config(\n",
    "    trial_path=trial_path,\n",
    "    output_path=f\"{trial_path}/best_pipeline.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dashboard 실행\n",
    "!autorag dashboard --trial_dir ./benchmark/test_1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#streamlit 실행\n",
    "!autorag run_web --trial_path ./benchmark/test_1/4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
