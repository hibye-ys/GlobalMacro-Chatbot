{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate testdataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime\n",
    "from DataProcessing.utils import load_yaml\n",
    "from DataProcessing.extract_graphstate import (\n",
    "    extract_documents_for_singlestore,\n",
    ")\n",
    "\n",
    "config = load_yaml(\"../config/embedding.yaml\")\n",
    "category_id = config[\"settings\"][\"category_id\"]\n",
    "filetype = config[\"settings\"][\"filetype\"]\n",
    "edit_path = config[\"settings\"][\"edit_path\"]\n",
    "# path = os.path.join(edit_path, category_id, \"json\")\n",
    "output_path = config[\"settings\"][\"output_path\"]\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_json_files(path):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            with open(os.path.join(path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for category in config[\"settings\"][\"category_id\"]:\n",
    "    category_path = os.path.join(edit_path, category, \"json\")\n",
    "    data_list.extend(load_json_files(category_path))\n",
    "\n",
    "\n",
    "all_documents = []\n",
    "for data in data_list:\n",
    "    documents = extract_documents_for_singlestore(data)\n",
    "    all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_documents)):\n",
    "    filename = all_documents[i].metadata[\"doc_id\"].split(\"_\")[0]\n",
    "    all_documents[i].metadata[\"filename\"] = filename\n",
    "\n",
    "for doc in all_documents:\n",
    "    doc.metadata = {k: v for k, v in doc.metadata.items() if v != [] and v != \"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import random\n",
    "import pandas as pd\n",
    "from autorag.data.utils.util import corpus_df_to_langchain_documents\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.56)\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "\n",
    "corpus_df = pd.read_parquet(\"./data/filtered_corpus.parquet\")\n",
    "langchain_docs = corpus_df_to_langchain_documents(corpus_df)\n",
    "random_samples = random.sample(langchain_docs, 100)\n",
    "\n",
    "\"\"\"language = \"korean\"\n",
    "\n",
    "generator.adapt(\n",
    "    language, evolutions=[simple, reasoning, multi_context], cache_dir=\"./data/cache\"\n",
    ")\n",
    "generator.save(evolutions=[simple, reasoning, multi_context], cache_dir=\"./data/cache\")\"\"\"\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    random_samples,\n",
    "    test_size=10,\n",
    "    distributions={simple: 0.25, reasoning: 0.25, multi_context: 0.5},\n",
    ")\n",
    "testset.to_pandas().to_csv(\"./data/ragas2_testset_10_2.csv\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorag.utils.preprocess import cast_qa_dataset\n",
    "import uuid\n",
    "\n",
    "result_df = pd.DataFrame(\n",
    "    {\n",
    "        \"qid\": [str(uuid.uuid4()) for _ in range(len(testset))],\n",
    "        \"query\": testset[\"question\"].tolist(),\n",
    "        \"generation_gt\": list(map(lambda x: x, testset[\"ground_truth\"].tolist())),\n",
    "    }\n",
    ")\n",
    "\n",
    "result_df[\"retrieval_gt\"] = testset[\"metadata\"].apply(\n",
    "    lambda x: list(map(lambda y: y[\"filename\"], x))\n",
    ")\n",
    "result_df = cast_qa_dataset(result_df)\n",
    "result_df.to_parquet(\"./data/use_splited_qa_ragas2_10_0.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from langchain_upstage.embeddings import UpstageEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def run_evaluate(json_path: str, testcount: int = 0):\n",
    "    load_dotenv()\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    filename = os.path.basename(json_path).split(\".\")[0]\n",
    "\n",
    "    dataset = Dataset.from_dict(json_data)\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset=dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "\n",
    "    df = result.to_pandas()\n",
    "    df.to_csv(f\"./data/results/result_{filename}_{testcount}.csv\")\n",
    "\n",
    "\n",
    "base_path = \"./data\"\n",
    "json_list = [x for x in os.listdir(base_path) if x.startswith(\"customtestset_\")]\n",
    "print(json_list)\n",
    "for path in json_list:\n",
    "    file_path = os.path.join(base_path, path)\n",
    "    run_evaluate(file_path, 0)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base_path = \"./data/results\"\n",
    "result_list = os.listdir(base_path)\n",
    "\n",
    "\n",
    "def print_describe(base_path, filename):\n",
    "    path = os.path.join(base_path, filename)\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    df = df[[\"context_precision\", \"context_recall\", \"faithfulness\", \"answer_relevancy\"]]\n",
    "    return df.agg([\"mean\", \"std\"])\n",
    "\n",
    "\n",
    "result_df_mean = pd.DataFrame()\n",
    "result_df_std = pd.DataFrame()\n",
    "\n",
    "for file in result_list:\n",
    "    if file.startswith(\"result_\"):\n",
    "        result = print_describe(base_path, file)\n",
    "        file_name = file.replace(\"result_customtestset_\", \"\").replace(\"_0.csv\", \"\")\n",
    "\n",
    "        result_mean = result.loc[\"mean\"]\n",
    "        result_mean[\"파일명\"] = file_name\n",
    "        result_df_mean = pd.concat([result_df_mean, result_mean.to_frame().T])\n",
    "\n",
    "        result_std = result.loc[\"std\"]\n",
    "        result_std[\"파일명\"] = file_name\n",
    "        result_df_std = pd.concat([result_df_std, result_std.to_frame().T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>파일명</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.547473</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>sonnet_multivector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307143</td>\n",
       "      <td>0.453698</td>\n",
       "      <td>0.151005</td>\n",
       "      <td>sonnet_parentdocument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.966393</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.801328</td>\n",
       "      <td>0.046288</td>\n",
       "      <td>sonnet_bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.238889</td>\n",
       "      <td>0.627508</td>\n",
       "      <td>0.09526</td>\n",
       "      <td>mini_multivector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.469314</td>\n",
       "      <td>0.107169</td>\n",
       "      <td>mini_parentdocument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.966393</td>\n",
       "      <td>0.511667</td>\n",
       "      <td>0.736923</td>\n",
       "      <td>0.055993</td>\n",
       "      <td>mini_bm25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context_precision context_recall faithfulness answer_relevancy  \\\n",
       "mean               1.0           0.12     0.547473         0.040684   \n",
       "mean               1.0       0.307143     0.453698         0.151005   \n",
       "mean          0.966393            0.4     0.801328         0.046288   \n",
       "mean               1.0       0.238889     0.627508          0.09526   \n",
       "mean               1.0       0.357143     0.469314         0.107169   \n",
       "mean          0.966393       0.511667     0.736923         0.055993   \n",
       "\n",
       "                        파일명  \n",
       "mean     sonnet_multivector  \n",
       "mean  sonnet_parentdocument  \n",
       "mean            sonnet_bm25  \n",
       "mean       mini_multivector  \n",
       "mean    mini_parentdocument  \n",
       "mean              mini_bm25  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>파일명</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315524</td>\n",
       "      <td>0.417318</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>sonnet_multivector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364107</td>\n",
       "      <td>0.435119</td>\n",
       "      <td>0.12878</td>\n",
       "      <td>sonnet_parentdocument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074134</td>\n",
       "      <td>0.442217</td>\n",
       "      <td>0.325952</td>\n",
       "      <td>0.090583</td>\n",
       "      <td>sonnet_bm25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38873</td>\n",
       "      <td>0.392216</td>\n",
       "      <td>0.109286</td>\n",
       "      <td>mini_multivector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417801</td>\n",
       "      <td>0.43134</td>\n",
       "      <td>0.12229</td>\n",
       "      <td>mini_parentdocument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074134</td>\n",
       "      <td>0.464549</td>\n",
       "      <td>0.313787</td>\n",
       "      <td>0.09633</td>\n",
       "      <td>mini_bm25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    context_precision context_recall faithfulness answer_relevancy  \\\n",
       "std               0.0       0.315524     0.417318         0.075617   \n",
       "std               0.0       0.364107     0.435119          0.12878   \n",
       "std          0.074134       0.442217     0.325952         0.090583   \n",
       "std               0.0        0.38873     0.392216         0.109286   \n",
       "std               0.0       0.417801      0.43134          0.12229   \n",
       "std          0.074134       0.464549     0.313787          0.09633   \n",
       "\n",
       "                       파일명  \n",
       "std     sonnet_multivector  \n",
       "std  sonnet_parentdocument  \n",
       "std            sonnet_bm25  \n",
       "std       mini_multivector  \n",
       "std    mini_parentdocument  \n",
       "std              mini_bm25  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
