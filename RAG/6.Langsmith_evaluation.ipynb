{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "client = Client()\n",
    "dataset_name = \"globalmacro_custom_dataset\"\n",
    "df = pd.read_excel(\"./data/custom_testdataset.xlsx\")\n",
    "\n",
    "\n",
    "def create_dataset(client, dataset_name, description=None):\n",
    "    for dataset in client.list_datasets():\n",
    "        if dataset.name == dataset_name:\n",
    "            return dataset\n",
    "\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=description,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = create_dataset(client, dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in df[\"question\"].tolist()],\n",
    "    outputs=[{\"answer\": a} for a in df[\"ground_truth\"].tolist()],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init_pinecone_index]\n",
      "{'dimension': 4096,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'financical-data-00': {'vector_count': 2012}},\n",
      " 'total_vector_count': 2012}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_teddynote.community.pinecone import init_pinecone_index\n",
    "from langchain_upstage.embeddings import UpstageEmbeddings\n",
    "from langchain_teddynote.community.pinecone import PineconeKiwiHybridRetriever\n",
    "from langchain_teddynote.korean import stopwords\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_params = init_pinecone_index(\n",
    "    index_name=\"globalmacro-chatbot\",\n",
    "    namespace=\"financical-data-00\",\n",
    "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
    "    sparse_encoder_path=\"../data/sparse_encoder_01.pkl\",\n",
    "    stopwords=stopwords(),\n",
    "    tokenizer=\"kiwi\",\n",
    "    embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large-query\"),\n",
    "    top_k=10,\n",
    "    alpha=0.4,  # alpha=0.75로 설정한 경우, (0.75: Dense Embedding, 0.25: Sparse Embedding)\n",
    ")\n",
    "\n",
    "\n",
    "pinecone_retriever = PineconeKiwiHybridRetriever(**pinecone_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from DataProcessing.utils import load_yaml\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "\n",
    "def create_chain(reranker=None, model=\"claude\"):\n",
    "    prompt_template = load_yaml(\"../prompts/Retriever._prompt.yaml\")[\"prompt\"]\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    if model == \"claude\":\n",
    "        llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0.5)\n",
    "    else:\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "    if reranker is not None:\n",
    "        if reranker == \"cohere\":\n",
    "            compressor = CohereRerank(model=\"rerank-multilingual-v3.0\")\n",
    "        elif reranker == \"jina\":\n",
    "            compressor = JinaRerank(model=\"jina-reranker-v2-base-multilingual\", top_n=5)\n",
    "        elif reranker == \"bge\":\n",
    "            model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "            compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "        elif reranker == \"ko-reranker\":\n",
    "            model = HuggingFaceCrossEncoder(model_name=\"Dongjin-kr/ko-reranker\")\n",
    "            compressor = CrossEncoderReranker(model=model, top_n=5)\n",
    "\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor, base_retriever=pinecone_retriever\n",
    "        )\n",
    "        retriever = (\n",
    "            compression_retriever if reranker is not None else pinecone_retriever\n",
    "        )\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return retriever, rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-as-Judge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_list = [None, \"cohere\", \"jina\", \"bge\", \"ko-reranker\"]\n",
    "reranker = \"ko-reranker\"\n",
    "model = \"openai\"\n",
    "retriever, chain = create_chain(reranker=reranker, model=model)\n",
    "dataset_name = \"globalmacro_cutom_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answer Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}\n",
    "\n",
    "\n",
    "qa_evalulator = LangChainStringEvaluator(\"qa\")\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[qa_evalulator],\n",
    "    experiment_prefix=f\"qa_{reranker}\",\n",
    "    metadata={\n",
    "        \"variant\": \"QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context QA Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
    "\n",
    "\n",
    "def context_answer_rag_answer(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    return {\n",
    "        \"context\": \"\\n\".join([doc.page_content for doc in context]),\n",
    "        \"answer\": chain.invoke(inputs[\"question\"]),\n",
    "        \"query\": inputs[\"question\"],\n",
    "    }\n",
    "\n",
    "\n",
    "eval_llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o-mini\")\n",
    "\n",
    "cot_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"cot_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    "    config={\"llm\": eval_llm},\n",
    ")\n",
    "\n",
    "\n",
    "context_qa_evaluator = LangChainStringEvaluator(\n",
    "    \"context_qa\",\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"context\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    "    config={\"llm\": eval_llm},\n",
    ")\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    context_answer_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[cot_qa_evaluator, context_qa_evaluator],\n",
    "    experiment_prefix=f\"cot_qa_{reranker}\",\n",
    "    metadata={\n",
    "        \"variant\": \"COT_QA & Context_QA Evaluator 를 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding distance Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # cuda, cpu\n",
    "    # encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# 임베딩 모델 평가자 생성\n",
    "hf_embedding_evaluator = LangChainStringEvaluator(\n",
    "    \"embedding_distance\",\n",
    "    config={\n",
    "        \"embeddings\": hf_embeddings,\n",
    "        \"distance_metric\": \"cosine\",  # \"cosine\", \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"\n",
    "    },\n",
    ")\n",
    "\n",
    "upstage_embedding_evaluator = LangChainStringEvaluator(\n",
    "    \"embedding_distance\",\n",
    "    config={\n",
    "        \"embeddings\": UpstageEmbeddings(model=\"solar-embedding-1-large-query\"),\n",
    "        \"distance_metric\": \"euclidean\",  # \"cosine\", \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"\n",
    "    },\n",
    ")\n",
    "\n",
    "openai_embedding_evaluator = LangChainStringEvaluator(\n",
    "    \"embedding_distance\",\n",
    "    config={\n",
    "        \"embeddings\": OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "        \"distance_metric\": \"euclidean\",  # \"cosine\", \"euclidean\", \"chebyshev\", \"hamming\", and \"manhattan\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 metric에 여거해긔 embedding 모델이 사용되는 경우, 결과는 평균값으로 산정\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        hf_embedding_evaluator,\n",
    "        upstage_embedding_evaluator,\n",
    "        openai_embedding_evaluator,\n",
    "    ],\n",
    "    experiment_prefix=f\"EMBEDDING-EVAL_{reranker}\",\n",
    "    metadata={\n",
    "        \"variant\": \"embedding_distance 활용한 평가\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groundedness Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "\n",
    "def ask_question(inputs: dict):\n",
    "    context = retriever.invoke(inputs[\"question\"])\n",
    "    context = \"\\n\".join([doc.page_content for doc in context])\n",
    "    return {\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"context\": context,\n",
    "        \"answer\": chain.invoke(inputs[\"question\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def upstage_groundness_check_evaluator(run: Run, example: Example) -> dict:\n",
    "    answer = run.outputs.get(\"answer\", \"\")\n",
    "    context = run.outputs.get(\"context\", \"\")\n",
    "    upstage_groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "    groundedness_score = upstage_groundedness_check.invoke(\n",
    "        {\"answer\": answer, \"context\": context}\n",
    "    )\n",
    "    groundedness_score = groundedness_score == \"grounded\"\n",
    "\n",
    "    return {\"key\": \"groundness_score\", \"score\": int(groundedness_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "from langchain_teddynote.evaluator import GroundnessChecker\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "groundedness_check = GroundnessChecker(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    ").create()\n",
    "\n",
    "\n",
    "def teddynote_groundness_check_evaluator(run: Run, example: Example) -> dict:\n",
    "    answer = run.outputs.get(\"answer\", \"\")\n",
    "    context = run.outputs.get(\"context\", \"\")\n",
    "\n",
    "    groundedness_score = groundedness_check.invoke(\n",
    "        {\"answer\": answer, \"context\": context}\n",
    "    )\n",
    "    groundedness_score = groundedness_score.score == \"yes\"\n",
    "\n",
    "    return {\"key\": \"groundness_score\", \"score\": int(groundedness_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=[\n",
    "        upstage_groundness_check_evaluator,\n",
    "        teddynote_groundness_check_evaluator,\n",
    "    ],\n",
    "    experiment_prefix=f\"GROUNDEDNESS-EVAL_{reranker}\",\n",
    "    metadata={\n",
    "        \"variant\": \"Upstage & teddynote Groundness Checker 를 활용한 Hallucination 평가\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macroagent-withrag-bsFmSw79-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
